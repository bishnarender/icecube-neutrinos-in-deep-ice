{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "05edeb7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e241b7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_meta.parquet is quite large and takes a long time to load, and consumes a lot of memory while you are \n",
    "#           holding it. Since it is a parquet file (and the row group is quite large), you cannot easily read it \n",
    "#           batch by batch. So create a dataset that split this file into 660 files, one for each batch.\n",
    "\n",
    "DATA_DIR='data'\n",
    "OUTPUT_DIR='data_'\n",
    "META_FILE = os.path.join(DATA_DIR, 'train_meta.parquet')\n",
    "start=time.time()            \n",
    "meta_train = pd.read_parquet(os.path.join(DATA_DIR, 'train_meta.parquet'))\n",
    "# meta_train.head(3) => \n",
    "#    batch_id  event_id  first_pulse_index  last_pulse_index   azimuth    zenith\n",
    "# 0         1        24                  0                60  5.029555  2.087498\n",
    "# 1         1        41                 61               111  0.417742  1.549686\n",
    "# 2         1        59                112               147  1.160466  2.401942\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9aef31c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2746.3 wrote data_/meta/train_meta_1.parquet\n",
      "  2746.4 wrote data_/meta/train_meta_2.parquet\n",
      "  2746.6 wrote data_/meta/train_meta_3.parquet\n",
      "  2746.7 wrote data_/meta/train_meta_4.parquet\n",
      "  2746.8 wrote data_/meta/train_meta_5.parquet\n",
      "  2746.9 wrote data_/meta/train_meta_6.parquet\n",
      "  2747.1 wrote data_/meta/train_meta_7.parquet\n",
      "  2747.2 wrote data_/meta/train_meta_8.parquet\n",
      "  2747.3 wrote data_/meta/train_meta_9.parquet\n",
      "  2747.4 wrote data_/meta/train_meta_10.parquet\n",
      "  2747.7 wrote data_/meta/train_meta_11.parquet\n",
      "  2747.8 wrote data_/meta/train_meta_12.parquet\n",
      "  2748.0 wrote data_/meta/train_meta_13.parquet\n",
      "  2748.1 wrote data_/meta/train_meta_14.parquet\n",
      "  2748.4 wrote data_/meta/train_meta_15.parquet\n",
      "  2748.5 wrote data_/meta/train_meta_16.parquet\n",
      "  2748.8 wrote data_/meta/train_meta_17.parquet\n",
      "  2749.0 wrote data_/meta/train_meta_18.parquet\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_meta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     14\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst_pulse_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlast_pulse_index\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mazimuth\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzenith\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n\u001b[0;32m---> 15\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m8.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m wrote \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/base_4/lib/python3.8/site-packages/pandas/core/frame.py:2889\u001b[0m, in \u001b[0;36mDataFrame.to_parquet\u001b[0;34m(self, path, engine, compression, index, partition_cols, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m   2802\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2803\u001b[0m \u001b[38;5;124;03mWrite a DataFrame to the binary parquet format.\u001b[39;00m\n\u001b[1;32m   2804\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2885\u001b[0m \u001b[38;5;124;03m>>> content = f.read()\u001b[39;00m\n\u001b[1;32m   2886\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2887\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparquet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_parquet\n\u001b[0;32m-> 2889\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mto_parquet\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2890\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2897\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2898\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/base_4/lib/python3.8/site-packages/pandas/io/parquet.py:411\u001b[0m, in \u001b[0;36mto_parquet\u001b[0;34m(df, path, engine, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m impl \u001b[38;5;241m=\u001b[39m get_engine(engine)\n\u001b[1;32m    409\u001b[0m path_or_buf: FilePath \u001b[38;5;241m|\u001b[39m WriteBuffer[\u001b[38;5;28mbytes\u001b[39m] \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mBytesIO() \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m path\n\u001b[0;32m--> 411\u001b[0m \u001b[43mimpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    414\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    415\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpartition_cols\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpartition_cols\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    417\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, io\u001b[38;5;241m.\u001b[39mBytesIO)\n",
      "File \u001b[0;32m~/miniconda3/envs/base_4/lib/python3.8/site-packages/pandas/io/parquet.py:189\u001b[0m, in \u001b[0;36mPyArrowImpl.write\u001b[0;34m(self, df, path, compression, index, storage_options, partition_cols, **kwargs)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi\u001b[38;5;241m.\u001b[39mparquet\u001b[38;5;241m.\u001b[39mwrite_to_dataset(\n\u001b[1;32m    181\u001b[0m             table,\n\u001b[1;32m    182\u001b[0m             path_or_handle,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;66;03m# write to single output file\u001b[39;00m\n\u001b[0;32m--> 189\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath_or_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/base_4/lib/python3.8/site-packages/pyarrow/parquet/core.py:3106\u001b[0m, in \u001b[0;36mwrite_table\u001b[0;34m(table, where, row_group_size, version, use_dictionary, compression, write_statistics, use_deprecated_int96_timestamps, coerce_timestamps, allow_truncated_timestamps, data_page_size, flavor, filesystem, compression_level, use_byte_stream_split, column_encoding, data_page_version, use_compliant_nested_type, encryption_properties, write_batch_size, dictionary_pagesize_limit, store_schema, **kwargs)\u001b[0m\n\u001b[1;32m   3083\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   3084\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ParquetWriter(\n\u001b[1;32m   3085\u001b[0m             where, table\u001b[38;5;241m.\u001b[39mschema,\n\u001b[1;32m   3086\u001b[0m             filesystem\u001b[38;5;241m=\u001b[39mfilesystem,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3104\u001b[0m             store_schema\u001b[38;5;241m=\u001b[39mstore_schema,\n\u001b[1;32m   3105\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[0;32m-> 3106\u001b[0m         \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3107\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   3108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path_like(where):\n",
      "File \u001b[0;32m~/miniconda3/envs/base_4/lib/python3.8/site-packages/pyarrow/parquet/core.py:1092\u001b[0m, in \u001b[0;36mParquetWriter.write_table\u001b[0;34m(self, table, row_group_size)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTable schema does not match schema used to create file: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1088\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mtable:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m vs. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mfile:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{!s}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1089\u001b[0m            \u001b[38;5;241m.\u001b[39mformat(table\u001b[38;5;241m.\u001b[39mschema, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema))\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 1092\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow_group_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrow_group_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for idx, df in meta_train.groupby('batch_id'):\n",
    "    # idx => 1, 2, and so on.\n",
    "\n",
    "    # type(df) => <class 'pandas.core.frame.DataFrame'>\n",
    "    \n",
    "    # df.head(3) =>                   (when idx=2)\n",
    "    #          batch_id  event_id  first_pulse_index  last_pulse_index   azimuth          zenith  \n",
    "    # 200000         2   3266199                  0                73  0.523428       2.094954  \n",
    "    # 200001         2   3266228                 74               149  3.125242       2.695349  \n",
    "    # 200002         2   3266282                150               191  5.720676       2.581088  \n",
    "\n",
    "    \n",
    "    file = os.path.join(OUTPUT_DIR, 'meta', f'train_meta_{idx}.parquet')\n",
    "    df = df[['event_id', 'first_pulse_index', 'last_pulse_index', 'azimuth', 'zenith']]\n",
    "    df.to_parquet(file)\n",
    "\n",
    "    print(f'{time.time()-start:8.1f} wrote {file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666d98ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e28c5a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/660 [00:00<?, ?it/s]prepare_data.py:32: DeprecationWarning: `groupby` is deprecated. It has been renamed to `group_by`.\n",
      "  df = df.groupby(\"event_id\").agg([pl.count()])\n",
      "100%|█████████████████████████████████████████| 660/660 [09:48<00:00,  1.12it/s]\n"
     ]
    }
   ],
   "source": [
    "!python prepare_data.py config.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2811afd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "91cfd198",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ae7650",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bc80278a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2023-10-28 17:43:10 - NodesAsPulses.__init__ - Writing log to \u001b[1mlogs/graphnet_20231028-174310.log\u001b[0m\n",
      "Training with the following configuration:\n",
      "{\n",
      "    \"SELECTION\": \"total\",\n",
      "    \"OUT\": \"B_MODEL_32\",\n",
      "    \"PATH\": \"data/\",\n",
      "    \"NUM_WORKERS\": 4,\n",
      "    \"SEED\": 2023,\n",
      "    \"BS\": 64,\n",
      "    \"BS_VALID\": 32,\n",
      "    \"L\": 192,\n",
      "    \"L_VALID\": 192,\n",
      "    \"EPOCHS\": 8,\n",
      "    \"LR_MAX\": 0.0005,\n",
      "    \"MODEL\": \"DeepIceModel\",\n",
      "    \"MOMS\": false,\n",
      "    \"DIV\": 25,\n",
      "    \"DIV_FINAL\": 25,\n",
      "    \"EMA\": false,\n",
      "    \"MODEL_KWARGS\": {\n",
      "        \"dim\": 384,\n",
      "        \"dim_base\": 128,\n",
      "        \"depth\": 12,\n",
      "        \"head_size\": 32\n",
      "    },\n",
      "    \"WEIGHTS\": false,\n",
      "    \"LOSS_FUNC\": \"loss_vms\",\n",
      "    \"METRIC\": \"loss\"\n",
      "}\n",
      "_______________________________________________________\n",
      "epoch     train_loss  valid_loss  loss      time    \n",
      "^Coch 1/8 : |█-------------------| 6.82% [17431/255770 2:24:17<32:52:54 2.0548]\n",
      "Traceback (most recent call last):\n",
      "  File \"train.py\", line 198, in <module>\n",
      "    main()\n",
      "  File \"train.py\", line 194, in main\n",
      "    train(config_data)\n",
      "  File \"train.py\", line 138, in train\n",
      "    learn.fit_one_cycle( \n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/callback/schedule.py\", line 119, in fit_one_cycle\n",
      "    self.fit(n_epoch, cbs=ParamScheduler(scheds)+L(cbs), reset_opt=reset_opt, wd=wd, start_epoch=start_epoch)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 264, in fit\n",
      "    self._with_events(self._do_fit, 'fit', CancelFitException, self._end_cleanup)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 253, in _do_fit\n",
      "    self._with_events(self._do_epoch, 'epoch', CancelEpochException)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 247, in _do_epoch\n",
      "    self._do_epoch_train()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 239, in _do_epoch_train\n",
      "    self._with_events(self.all_batches, 'train', CancelTrainException)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 205, in all_batches\n",
      "    for o in enumerate(self.dl): self.one_batch(*o)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 235, in one_batch\n",
      "    self._with_events(self._do_one_batch, 'batch', CancelBatchException)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 223, in _do_one_batch\n",
      "    self._do_grad_opt()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 211, in _do_grad_opt\n",
      "    self._with_events(self._backward, 'backward', CancelBackwardException)\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 199, in _with_events\n",
      "    try: self(f'before_{event_type}');  f()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/fastai/learner.py\", line 207, in _backward\n",
      "    def _backward(self): self.loss_grad.backward()\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/na/miniconda3/envs/base_4/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# download graphnet from: https://github.com/graphnet-team/graphnet/tree/main   (as installation is complicated)\n",
    " \n",
    "# \"pytorch_scatter\"/\"torch_scatter\" =>                       (https://data.pyg.org/whl/) (https://github.com/rusty1s/pytorch_scatter)\n",
    "# pip3 install https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_scatter-2.1.2%2Bpt20cu118-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "# \"torch_cluster\" =>\n",
    "# pip3 install https://data.pyg.org/whl/torch-2.0.0%2Bcu118/torch_cluster-1.6.3%2Bpt20cu118-cp38-cp38-linux_x86_64.whl\n",
    "\n",
    "# B model 32\n",
    "!python train.py config.json \\\n",
    "        MODEL DeepIceModel \\\n",
    "        MODEL_KWARGS.dim 384 \\\n",
    "        MODEL_KWARGS.dim_base 128 \\\n",
    "        MODEL_KWARGS.depth 12 \\\n",
    "        MODEL_KWARGS.head_size 32 \\\n",
    "        OUT B_MODEL_32 \\\n",
    "        LR_MAX 5e-4 \\\n",
    "        MOMS false \\\n",
    "        LOSS_FUNC loss_vms \\\n",
    "        BS 64 # batch_size (default: 32)\n",
    "# OUT is output path\n",
    "# LOSS_FUNC after reaching near the peak = loss_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367a58fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ccca87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b2c21a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2023-10-17 22:26:02 - NodesAsPulses.__init__ - Writing log to \u001b[1mlogs/graphnet_20231017-222602.log\u001b[0m\r\n",
      "Training with the following configuration:\r\n",
      "{\r\n",
      "    \"SELECTION\": \"total\",\r\n",
      "    \"OUT\": \"B_MODEL_B_64\",\r\n",
      "    \"PATH\": \"data/\",\r\n",
      "    \"NUM_WORKERS\": 4,\r\n",
      "    \"SEED\": 2023,\r\n",
      "    \"BS\": 2,\r\n",
      "    \"BS_VALID\": 32,\r\n",
      "    \"L\": 192,\r\n",
      "    \"L_VALID\": 192,\r\n",
      "    \"EPOCHS\": 8,\r\n",
      "    \"LR_MAX\": 0.0001,\r\n",
      "    \"MODEL\": \"DeepIceModel\",\r\n",
      "    \"MOMS\": false,\r\n",
      "    \"DIV\": 25,\r\n",
      "    \"DIV_FINAL\": 25,\r\n",
      "    \"EMA\": false,\r\n",
      "    \"MODEL_KWARGS\": {\r\n",
      "        \"dim\": 768,\r\n",
      "        \"dim_base\": 192,\r\n",
      "        \"depth\": 12,\r\n",
      "        \"head_size\": 32,\r\n",
      "        \"n_rel\": 4\r\n",
      "    },\r\n",
      "    \"WEIGHTS\": false,\r\n",
      "    \"LOSS_FUNC\": \"loss_vms\",\r\n",
      "    \"METRIC\": \"loss\"\r\n",
      "}\r\n",
      "_______________________________________________________\r\n"
     ]
    }
   ],
   "source": [
    "# B model 4 REL\n",
    "!python train.py config.json \\\n",
    "       MODEL DeepIceModel \\\n",
    "       MODEL_KWARGS.dim 768 \\\n",
    "       MODEL_KWARGS.dim_base 192 \\\n",
    "       MODEL_KWARGS.depth 12 \\\n",
    "       MODEL_KWARGS.head_size 32 \\\n",
    "       MODEL_KWARGS.n_rel 4 \\\n",
    "       OUT B_MODEL_B_64_REL \\\n",
    "       LR_MAX 1e-4 \\\n",
    "       MOMS false  \\\n",
    "       BS 64 # batch_size (default: 32)    \n",
    "# OUT is output path\n",
    "# LOSS_FUNC after reaching near the peak = loss_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdfcb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "29c4c2e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2023-10-17 22:18:11 - NodesAsPulses.__init__ - Writing log to \u001b[1mlogs/graphnet_20231017-221811.log\u001b[0m\r\n",
      "Training with the following configuration:\r\n",
      "{\r\n",
      "    \"SELECTION\": \"total\",\r\n",
      "    \"OUT\": \"B_MODEL_B_64\",\r\n",
      "    \"PATH\": \"data/\",\r\n",
      "    \"NUM_WORKERS\": 4,\r\n",
      "    \"SEED\": 2023,\r\n",
      "    \"BS\": 2,\r\n",
      "    \"BS_VALID\": 32,\r\n",
      "    \"L\": 192,\r\n",
      "    \"L_VALID\": 192,\r\n",
      "    \"EPOCHS\": 8,\r\n",
      "    \"LR_MAX\": 0.0001,\r\n",
      "    \"MODEL\": \"EncoderWithDirectionReconstructionV22\",\r\n",
      "    \"MOMS\": false,\r\n",
      "    \"DIV\": 25,\r\n",
      "    \"DIV_FINAL\": 25,\r\n",
      "    \"EMA\": false,\r\n",
      "    \"MODEL_KWARGS\": {\r\n",
      "        \"dim\": 384,\r\n",
      "        \"dim_base\": 128,\r\n",
      "        \"depth\": 8,\r\n",
      "        \"head_size\": 32\r\n",
      "    },\r\n",
      "    \"WEIGHTS\": false,\r\n",
      "    \"LOSS_FUNC\": \"loss_vms\",\r\n",
      "    \"METRIC\": \"loss\"\r\n",
      "}\r\n",
      "_______________________________________________________\r\n"
     ]
    }
   ],
   "source": [
    "# S + GNN\n",
    "!python train.py config.json \\\n",
    "       MODEL EncoderWithDirectionReconstructionV22 \\\n",
    "       MODEL_KWARGS.dim 384 \\\n",
    "       MODEL_KWARGS.dim_base 128 \\\n",
    "       MODEL_KWARGS.depth 8 \\\n",
    "       MODEL_KWARGS.head_size 32 \\\n",
    "       OUT S_MODEL_GNN_64 \\\n",
    "       LR_MAX 1e-4 \\\n",
    "       MOMS false  \\\n",
    "       BS 64 # batch_size (default: 32)    \n",
    "\n",
    "# OUT is output path\n",
    "# LOSS_FUNC after reaching near the peak = loss_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b9730",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aa796531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;34mgraphnet\u001b[0m [MainProcess] \u001b[32mINFO    \u001b[0m 2023-10-17 22:20:20 - NodesAsPulses.__init__ - Writing log to \u001b[1mlogs/graphnet_20231017-222020.log\u001b[0m\r\n",
      "Training with the following configuration:\r\n",
      "{\r\n",
      "    \"SELECTION\": \"total\",\r\n",
      "    \"OUT\": \"B_MODEL_B_64\",\r\n",
      "    \"PATH\": \"data/\",\r\n",
      "    \"NUM_WORKERS\": 4,\r\n",
      "    \"SEED\": 2023,\r\n",
      "    \"BS\": 2,\r\n",
      "    \"BS_VALID\": 32,\r\n",
      "    \"L\": 192,\r\n",
      "    \"L_VALID\": 192,\r\n",
      "    \"EPOCHS\": 8,\r\n",
      "    \"LR_MAX\": 0.0001,\r\n",
      "    \"MODEL\": \"EncoderWithDirectionReconstructionV23\",\r\n",
      "    \"MOMS\": false,\r\n",
      "    \"DIV\": 25,\r\n",
      "    \"DIV_FINAL\": 25,\r\n",
      "    \"EMA\": false,\r\n",
      "    \"MODEL_KWARGS\": {\r\n",
      "        \"dim\": 768,\r\n",
      "        \"dim_base\": 128,\r\n",
      "        \"depth\": 12,\r\n",
      "        \"head_size\": 64\r\n",
      "    },\r\n",
      "    \"WEIGHTS\": false,\r\n",
      "    \"LOSS_FUNC\": \"loss_vms\",\r\n",
      "    \"METRIC\": \"loss\"\r\n",
      "}\r\n",
      "_______________________________________________________\r\n"
     ]
    }
   ],
   "source": [
    "# B + GNN\n",
    "!python train.py config.json \\\n",
    "       MODEL EncoderWithDirectionReconstructionV23 \\\n",
    "       MODEL_KWARGS.dim 768 \\\n",
    "       MODEL_KWARGS.dim_base 128 \\\n",
    "       MODEL_KWARGS.depth 12 \\\n",
    "       MODEL_KWARGS.head_size 64 \\\n",
    "       OUT B_MODEL_GNN_64 \\\n",
    "       LR_MAX 1e-4 \\\n",
    "       MOMS false  \\\n",
    "       BS 64 # batch_size (default: 32)    \n",
    "# OUT is output path\n",
    "# LOSS_FUNC after reaching near the peak = loss_comb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d44520",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
